{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "We removed all the stop words, punctuations and normalized the texts to lower case. \n",
    "\n",
    "\n",
    "### Model\n",
    "We will use Naive Bayes as our baseline model. \n",
    "\n",
    "Parametrization:\n",
    "\n",
    "We let $x_j$ denote the identity of the j-th word in a text response. Thus, xj is now an integer taking values in {1, . . . , |V|}, where |V| is the size of our vocabulary (dictionary). A sample of $d$ words is now represented by a vector $(x_1, x_2, . . . , x_d)$ of length d.\n",
    "\n",
    "Naive Bayes assumes that $p(x_j|y)$ are independent from each other. That $p(x_1,...x_d|y) = p(x_1|y)p(x_2|y)...p(x_d|y)$. We also asume that $x_j|y$ comes from a multinomial distribution, with parameters \n",
    "$\\phi_{k|y=1} (p(x_j = k\\,|\\,y=1))$ and $\\phi_{k|y=0} (p(x_j = k\\,|\\,y=0))$\n",
    "\n",
    "Then the likehood of the data is given by \n",
    "\n",
    "$$L(\\phi_y,\\phi_{k|y=1},\\phi_{k|y=0}) = \\prod_{i=1}^np(x^i,y^i) = \\prod_{i=1}^n \\big(\\prod_{j=1}^{d^i} p(x_j^{i}|y;\\phi_{k|y=0},\\phi_{k|y=1})\\big)p(y^i;\\phi_y)$$\n",
    "\n",
    "\n",
    "We find the MLE estimates of the parameters to fit the model.\n",
    "\n",
    "To find the most indicative words for treatment/control, we use the metric:\n",
    "\n",
    "$$log\\bigg(\\frac{P(word\\_i \\,|\\,treatment)}{P(word\\_i\\,|\\,control)}\\bigg)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 470 entries, 0 to 469\n",
      "Data columns (total 5 columns):\n",
      "arm                470 non-null object\n",
      "pre_q_original     470 non-null object\n",
      "post_q_original    470 non-null object\n",
      "pre_q              470 non-null object\n",
      "post_q             470 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 18.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "arm\n",
       "arch_graphic    94\n",
       "arch_strong     94\n",
       "arch_weak       94\n",
       "control         94\n",
       "curb_appeal     94\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the entire survey data\n",
    "df = pd.read_csv(\"data/study_data.csv\")\n",
    "df.info()\n",
    "df.groupby(['arm']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(message):\n",
    "    \"\"\"Get the normalized list of words from a message string.\n",
    "\n",
    "    This function should split a message into words, normalize them, and return\n",
    "    the resulting list. For splitting, you should split on spaces. For normalization,\n",
    "    you should convert everything to lowercase.\n",
    "\n",
    "    Args:\n",
    "        message: A string containing an SMS message\n",
    "\n",
    "    Returns:\n",
    "       The list of normalized words from the message.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    words = message.split()\n",
    "    normalized_words = list(map(lambda word:word.lower(),words))\n",
    "    return normalized_words\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "\n",
    "def create_dictionary(messages):\n",
    "    \"\"\"Create a dictionary mapping words to integer indices.\n",
    "\n",
    "    This function should create a dictionary of word to indices using the provided\n",
    "    training messages. Use get_words to process each message.\n",
    "\n",
    "    Rare words are often not useful for modeling. Please only add words to the dictionary\n",
    "    if they occur in at least five messages.\n",
    "\n",
    "    Args:\n",
    "        messages: A list of strings containing SMS messages\n",
    "\n",
    "    Returns:\n",
    "        A python dict mapping words to integers.\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    from collections import defaultdict\n",
    "    sentence_freq = defaultdict(int)\n",
    "    count = 0\n",
    "    word_int = {}\n",
    "    for i in range(len(messages)):\n",
    "        unique= defaultdict(int)\n",
    "\n",
    "        # get the unique words in the messages\n",
    "        for word in get_words(messages[i]):\n",
    "            unique[word]+=1\n",
    "        \n",
    "        # add the words to dictionary only if they appear in at least five messages\n",
    "        for unique_word in unique:\n",
    "            sentence_freq[unique_word]+=1\n",
    "            if sentence_freq[unique_word] ==9:\n",
    "                word_int[unique_word] = count\n",
    "                count+=1\n",
    "\n",
    "    return word_int\n",
    "\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "\n",
    "def transform_text(messages, word_dictionary):\n",
    "    \"\"\"Transform a list of text messages into a numpy array for further processing.\n",
    "\n",
    "    This function should create a numpy array that contains the number of times each word\n",
    "    of the vocabulary appears in each message. \n",
    "    Each row in the resulting array should correspond to each message \n",
    "    and each column should correspond to a word of the vocabulary.\n",
    "\n",
    "    Use the provided word dictionary to map words to column indices. Ignore words that\n",
    "    are not present in the dictionary. Use get_words to get the words for a message.\n",
    "\n",
    "    Args:\n",
    "        messages: A list of strings where each string is an SMS message.\n",
    "        word_dictionary: A python dict mapping words to integers.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array marking the words present in each message.\n",
    "        Where the component (i,j) is the number of occurrences of the\n",
    "        j-th vocabulary word in the i-th message.\n",
    "    \"\"\"\n",
    "    # *** START CODE HERE ***\n",
    "    table = np.zeros((len(messages),len(word_dictionary)))\n",
    "    all_words = list(word_dictionary.keys())\n",
    "\n",
    "    for i in range(len(messages)):\n",
    "        words = get_words(messages[i])\n",
    "        for j in range(len(word_dictionary)):\n",
    "            for word in words:\n",
    "                if word == all_words[j]:    \n",
    "                    table[i,j] +=1\n",
    "\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_naive_bayes_model(matrix, labels, treatment_label, control_label):\n",
    "    \"\"\"Fit a naive bayes model.\n",
    "\n",
    "    This function should fit a Naive Bayes model given a training matrix and labels.\n",
    "\n",
    "    The function should return the state of that model.\n",
    "\n",
    "    Feel free to use whatever datatype you wish for the state of the model.\n",
    "\n",
    "    Args:\n",
    "        matrix: A numpy array containing word counts for the training data\n",
    "        labels: The binary (0 or 1) labels for that training data\n",
    "\n",
    "    Returns: The trained model\n",
    "    \"\"\"\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    n_words = matrix.shape[1]\n",
    "\n",
    "    treatment = matrix[labels == treatment_label,:]\n",
    "    control = matrix[labels == control_label,:]\n",
    "\n",
    "    length_treatment = np.sum(treatment) #number of words in all spam texts\n",
    "    length_control = np.sum(control) #number of words in all non-spam texts\n",
    "\n",
    "    model = {}\n",
    "    model['prob_treatment'] = (np.sum(treatment,axis=0)+1)/(length_treatment+n_words)\n",
    "    model['prob_control'] = (np.sum(control,axis=0)+1)/(length_control+n_words)\n",
    "    model['phi'] = treatment.shape[0]/(treatment.shape[0]+control.shape[0])\n",
    "\n",
    "    return model\n",
    "\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "\n",
    "def predict_from_naive_bayes_model(model, matrix):\n",
    "    \"\"\"Use a Naive Bayes model to compute predictions for a target matrix.\n",
    "\n",
    "    This function should be able to predict on the models that fit_naive_bayes_model\n",
    "    outputs.\n",
    "\n",
    "    Args:\n",
    "        model: A trained model from fit_naive_bayes_model\n",
    "        matrix: A numpy array containing word counts\n",
    "\n",
    "    Returns: A numpy array containg the predictions from the model\n",
    "    \"\"\"\n",
    "    # *** START CODE HERE ***\n",
    "    log_prob_treatment = np.sum(np.log(model['prob_treatment'])*matrix,axis=1)\n",
    "    log_prob_control = np.sum(np.log(model['prob_control'])*matrix,axis=1)\n",
    "    phi = model['phi']\n",
    "    \n",
    "    prob = 1 / (1 + np.exp(log_prob_control + np.log(1-phi) - log_prob_treatment - np.log(phi)))\n",
    "    pred = np.zeros(len(prob))\n",
    "    pred[np.argwhere(prob>0.5)]=1\n",
    "    return pred\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "\n",
    "def get_top_naive_bayes_words(model, dictionary,num):\n",
    "    \"\"\"Compute the top five words that are most indicative of the treatment (i.e positive) class.\n",
    "\n",
    "    Return the words in sorted form, with the most indicative word first.\n",
    "\n",
    "    Args:\n",
    "        model: The Naive Bayes model returned from fit_naive_bayes_model\n",
    "        dictionary: A mapping of word to integer ids\n",
    "\n",
    "    Returns: A list of the top five most indicative words in sorted order with the most indicative first\n",
    "    \"\"\"\n",
    "    # *** START CODE HERE ***\n",
    "    log_treatment = np.log(model['prob_treatment'])\n",
    "    log_control = np.log(model['prob_control'])\n",
    "\n",
    "    to_sort = list(zip(log_treatment-log_control,dictionary.keys()))\n",
    "\n",
    "    # take first element for sort\n",
    "    def takeFirst(elem):\n",
    "        return elem[0]\n",
    "    \n",
    "    to_sort.sort(key=takeFirst,reverse=True)\n",
    "    top_five_words = list(zip(*to_sort))[1][0:num]\n",
    "    return top_five_words\n",
    "\n",
    "\n",
    "    # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanText(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = re.sub(r'[,!@#$%^&*)(|/><\";:.?\\'\\\\}{-]',\"\",text)\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nb (data, treatment_label,control_label,top_num):\n",
    "    \n",
    "    # pre-process data\n",
    "    treatment_control = df[df['arm'].isin([treatment_label,control_label])]\n",
    "    train_responses = np.array(treatment_control['post_q'].apply(cleanText))\n",
    "    train_labels = treatment_control['arm']\n",
    "    word_dict = create_dictionary(train_responses)\n",
    "    train_matrix = transform_text(train_responses,word_dict)\n",
    "    \n",
    "    # fit naive bayes and get training accuracy\n",
    "    naive_bayes_model = fit_naive_bayes_model(train_matrix, train_labels, treatment_label,control_label)\n",
    "    naive_bayes_predictions = predict_from_naive_bayes_model(naive_bayes_model, train_matrix)\n",
    "    \n",
    "    binary_train_labels = train_labels.copy()\n",
    "    binary_train_labels[train_labels == treatment_label] = 1\n",
    "    binary_train_labels[train_labels == control_label] = 0\n",
    "\n",
    "    naive_bayes_accuracy = np.mean(naive_bayes_predictions == binary_train_labels)\n",
    "    top_words = get_top_naive_bayes_words(naive_bayes_model, word_dict,top_num)\n",
    "    \n",
    "    return naive_bayes_accuracy, top_words\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.8563829787234043  \n",
      " Top indicative words are  ('georgian', 'symmetrical', 'multi', 'bay', 'pane', 'gabled', 'symmetry', 'also', 'over', 'design', 'feature', 'center', '5', 'door', 'stone')\n"
     ]
    }
   ],
   "source": [
    "# arch strong vs control\n",
    "acc, top_words = train_nb (df, 'arch_strong','control',15)\n",
    "print(\"Training accuracy is\", acc, \" \\n Top indicative words are \",top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.8191489361702128  \n",
      " Top indicative words are  ('georgian', 'triangular', 'decorative', 'symmetric', 'symmetrical', 'multi', 'paned', 'typical', 'center', 'open', 'pane', '5', 'style', 'stone', 'across')\n"
     ]
    }
   ],
   "source": [
    "# arch weak and control\n",
    "acc, top_words = train_nb (df, 'arch_weak','control',15)\n",
    "print(\"Training accuracy is\", acc, \" \\n Top indicative words are \",top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.8297872340425532  \n",
      " Top indicative words are  ('bay', 'gabled', 'dormer', 'colonial', 'four', 'facade', 'over', 'appear', '12', 'pane', 'floor', 'design', 'symmetry', 'seem', 'place')\n"
     ]
    }
   ],
   "source": [
    "# arch strong and arch weak\n",
    "acc, top_words = train_nb (df, 'arch_strong','arch_weak',15)\n",
    "print(\"Training accuracy is\", acc, \" \\n Top indicative words are \",top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.7606382978723404  \n",
      " Top indicative words are  ('curb', 'way', 'appeal', 'dull', 'can', 'entry', 'doesnt', 'do', 'flower', 'drab', 'exterior', 'plant', 'would', 'color', 'classic')\n"
     ]
    }
   ],
   "source": [
    "# curb appeal and control\n",
    "acc, top_words = train_nb (df, 'curb_appeal','control',15)\n",
    "print(\"Training accuracy is\", acc, \" \\n Top indicative words are \",top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.8297872340425532  \n",
      " Top indicative words are  ('curb', 'appeal', 'but', 'way', 'dull', 'much', 'lot', 'natural', 'landscape', 'frame', 'color', 'plant', 'welcome', 'not', 'you')\n"
     ]
    }
   ],
   "source": [
    "acc, top_words = train_nb (df, 'curb_appeal','arch_strong',15)\n",
    "print(\"Training accuracy is\", acc, \" \\n Top indicative words are \",top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will try the Bernoulli Naive Bayes model. It might outperform the multinomial event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "treatment_label = 'arch_strong'\n",
    "control_label = 'arch_weak'\n",
    "treatment_control = df[df['arm'].isin([treatment_label,control_label])]\n",
    "X = np.array(treatment_control['post_q'].apply(cleanText))\n",
    "y = treatment_control['arm']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5263157894736842\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  treatment       0.52      0.58      0.55        19\n",
      "    control       0.53      0.47      0.50        19\n",
      "\n",
      "avg / total       0.53      0.53      0.53        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=['treatment','control']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli NB actually outperforms multinomial NB. Therefore we will use Bernoulli NB as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.026686477312418 1800s -4.439835923799467 the\n",
      "-7.026686477312418 1880s -4.565165086412526 be\n",
      "-7.026686477312418 1940s -4.678453607717966 it\n",
      "-7.026686477312418 3rd -4.983291179932513 house\n",
      "-7.026686477312418 accentuate -5.033777745688296 have\n",
      "-7.026686477312418 adjoin -5.099620697480413 of\n",
      "-7.026686477312418 after -5.124033886222779 and\n",
      "-7.026686477312418 again -5.14688726437217 window\n",
      "-7.026686477312418 against -5.22601289569945 with\n",
      "-7.026686477312418 alabaster -5.275981774420433 this\n",
      "-7.026686477312418 align -5.276618697887858 in\n",
      "-7.026686477312418 alone -5.359216156053652 style\n",
      "-7.026686477312418 along -5.387432325418917 look\n",
      "-7.026686477312418 amazingly -5.393444339280163 roof\n",
      "-7.026686477312418 any -5.44848325276888 home\n",
      "-7.026686477312418 apart -5.45521509479523 front\n",
      "-7.026686477312418 apparent -5.504643033847124 its\n",
      "-7.026686477312418 appeal -5.506961491133849 on\n",
      "-7.026686477312418 appearance -5.511048216871162 georgian\n",
      "-7.026686477312418 architectural -5.534294996028426 triangular\n"
     ]
    }
   ],
   "source": [
    "def show_most_informative_features(vectorizer, clf, n=20):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "    for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "        print  (coef_1, fn_1, coef_2, fn_2)\n",
    "\n",
    "        \n",
    "clf = nb.named_steps['clf']        \n",
    "vect = nb.named_steps['vect']\n",
    "show_most_informative_features(vect,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 755)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
